{"cells": [{"cell_type": "markdown", "metadata": {"id": "view-in-github"}, "source": ["<a href=\"https://colab.research.google.com/github/lyco-p/Phos/blob/main/Phos_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Phos. Computational Optical Imaging\n", "\n", "**Phos** is a film simulation tool based on \"Computational optical imaging\". By calculating the transmission of light through film emulsion layers and simulating optical diffusion (bloom) and grain, it recreates the aesthetic of analog film.\n", "\n", "### Features\n", "- **GPU Acceleration**: Uses NVIDIA GPU (via CuPy) for high-speed processing.\n", "- **Film Presets**: NC200 (Color), AS100 (B&W), FS200 (High Contrast B&W).\n", "- **Customizable**: Adjust grain, tone mapping, and more.\n", "- **Video Support**: Process videos with frame-by-frame rendering.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#@title 1. Install Dependencies\n", "#@markdown Installs `cupy-cuda12x` for GPU acceleration and `opencv-python-headless`.\n", "\n", "!pip install cupy-cuda12x opencv-python-headless tqdm\n", "\n", "import cv2\n", "import numpy as np\n", "import os\n", "import sys\n", "import time\n", "import shutil\n", "import subprocess\n", "from google.colab import files\n", "from IPython.display import Image, display\n", "from tqdm.notebook import tqdm\n", "\n", "print(\"Dependencies installed.\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#@title 2. Setup Core Library\n", "#@markdown Creates the `core` package structure in the Colab environment.\n", "\n", "import os\n", "os.makedirs(\"core\", exist_ok=True)\n", "\n", "# Create __init__.py\n", "with open(\"core/__init__.py\", \"w\") as f:\n", "    f.write(\"\")\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%%writefile core/backend.py\nimport numpy as np\nimport cv2\nimport sys\n\n# Backend selection: CuPy > PyTorch (MPS/CUDA) > NumPy\n\nHAS_GPU = False\nBACKEND_TYPE = \"CPU\"\ncp = None\ntorch = None\n\n# 1. Try CuPy (best for NVIDIA)\ntry:\n    import cupy as cp\n    import cupyx.scipy.ndimage\n    HAS_GPU = True\n    BACKEND_TYPE = \"CUDA (CuPy)\"\nexcept ImportError:\n    pass\n\n# 2. Try PyTorch (best for Mac/MPS, or if CuPy missing on NVIDIA)\nif not HAS_GPU:\n    try:\n        import torch\n        import torchvision\n        if torch.backends.mps.is_available():\n            HAS_GPU = True\n            BACKEND_TYPE = \"MPS (PyTorch)\"\n        elif torch.cuda.is_available():\n            HAS_GPU = True\n            BACKEND_TYPE = \"CUDA (PyTorch)\"\n    except ImportError:\n        pass\n\n# -----------------------------------------------------------------------------\n# Adapter for PyTorch to mimic NumPy/CuPy API\n# -----------------------------------------------------------------------------\nclass TorchBackendAdapter:\n    def __init__(self, device):\n        self.device = device\n        self.float32 = torch.float32\n        self.uint8 = torch.uint8\n        \n    def asarray(self, arr):\n        if isinstance(arr, torch.Tensor):\n            return arr.to(self.device)\n        return torch.from_numpy(np.array(arr)).to(self.device)\n        \n    def asnumpy(self, arr):\n        if isinstance(arr, torch.Tensor):\n            return arr.cpu().numpy()\n        return np.array(arr)\n        \n    def mean(self, arr, axis=None):\n        return torch.mean(arr, dim=axis)\n        \n    def clip(self, arr, min_val, max_val):\n        if not isinstance(arr, torch.Tensor):\n            # Handle scalar input (mimic numpy.clip behavior for scalars)\n            return min(max(arr, min_val), max_val)\n        return torch.clamp(arr, min_val, max_val)\n        \n    def abs(self, arr):\n        return torch.abs(arr)\n        \n    def power(self, arr, exponent):\n        return torch.pow(arr, exponent)\n    \n    def maximum(self, a, b):\n        # b can be scalar\n        if isinstance(b, (int, float)):\n            b = torch.tensor(b, device=self.device, dtype=a.dtype)\n        return torch.maximum(a, b)\n        \n    def dstack(self, arrays):\n        return torch.dstack(arrays)\n        \n    # Nested Random module\n    class RandomAdapter:\n        def __init__(self, device):\n            self.device = device\n            \n        def normal(self, loc, scale, size):\n            # size is a tuple\n            return torch.normal(mean=loc, std=scale, size=size, device=self.device)\n            \n        def choice(self, a, size):\n            # simplified choice for [-1, 1] case usage in Phos\n            # xp.random.choice([-1, 1], shape)\n            if isinstance(a, list) and set(a) == {-1, 1}:\n                # Generate 0s and 1s, map to -1 and 1\n                # 0 -> -1, 1 -> 1\n                rand_bool = torch.randint(0, 2, size, device=self.device)\n                return rand_bool * 2 - 1\n            else:\n                # Fallback implementation for generic choice if needed\n                indices = torch.randint(0, len(a), size, device=self.device)\n                if isinstance(a, list):\n                    a_tensor = torch.tensor(a, device=self.device)\n                return a_tensor[indices]\n\n    @property\n    def random(self):\n        return self.RandomAdapter(self.device)\n\n    def astype(self, arr, dtype):\n        return arr.to(dtype)\n\n# -----------------------------------------------------------------------------\n# Initialize 'xp'\n# -----------------------------------------------------------------------------\nif BACKEND_TYPE.startswith(\"CUDA (CuPy)\"):\n    xp = cp\n    # Add loose function alias if not present (CuPy mirrors NumPy, so xp.array works, but xp.astype(arr) isn't standard numpy function, it's method)\n    # We need a unified function.\n    \n    def astype(arr, dtype):\n        return arr.astype(dtype)\n        \n    xp.astype = astype # Monkey patch or just export it? \n    # processing.py imports xp. We can add it to xp namespace effectively if we wrap it, but standard module doesn't allow easy assign.\n    # We should export `astype` from backend.py and use it.\n\n    def to_cpu(arr):\n        if isinstance(arr, cp.ndarray):\n            return cp.asnumpy(arr)\n        return arr\n\n    def to_gpu(arr):\n        if isinstance(arr, np.ndarray):\n            return cp.asarray(arr)\n        return arr\n\nelif \"PyTorch\" in BACKEND_TYPE:\n    if \"MPS\" in BACKEND_TYPE:\n        device = torch.device(\"mps\")\n    else:\n        device = torch.device(\"cuda\")\n        \n    xp = TorchBackendAdapter(device)\n    # Adapter already has astype method, so xp.astype works!\n    \n    def to_cpu(arr):\n        if isinstance(arr, torch.Tensor):\n            return arr.cpu().numpy()\n        return arr\n\n    def to_gpu(arr):\n        if isinstance(arr, np.ndarray):\n            return torch.from_numpy(arr).to(device)\n        return arr\n\nelse:\n    xp = np\n    \n    # NumPy doesn't have np.astype(arr, dtype).\n    def astype_numpy(arr, dtype):\n        return arr.astype(dtype)\n    xp.astype = astype_numpy\n    \n    def to_cpu(arr):\n        return arr\n\n    def to_gpu(arr):\n        return arr\n\n\n\ndef get_backend_name():\n    return BACKEND_TYPE\n\n\ndef gaussian_blur(arr, ksize, sigma):\n    \"\"\"\n    Apply Gaussian blur.\n    ksize: tuple (k_h, k_w).\n    sigma: float.\n    \"\"\"\n    if BACKEND_TYPE.startswith(\"CUDA (CuPy)\") and isinstance(arr, cp.ndarray):\n        if sigma <= 0:\n            k = ksize[0]\n            sigma = 0.3 * ((k - 1) * 0.5 - 1) + 0.8\n        return cupyx.scipy.ndimage.gaussian_filter(arr, sigma=sigma)\n        \n    elif \"PyTorch\" in BACKEND_TYPE and isinstance(arr, torch.Tensor):\n        import torchvision.transforms.functional as F\n        \n        if sigma <= 0:\n            k = ksize[0]\n            sigma = 0.3 * ((k - 1) * 0.5 - 1) + 0.8\n            \n        # F.gaussian_blur expects (C, H, W). Arr is (H, W) or (H, W, C)?\n        # Phos logic: Single channel lux map -> (H, W).\n        # We need to unsqueeze to (1, H, W) or (1, 1, H, W).\n        # GaussianBlur supports (..., H, W)\n        \n        # Ensure ksize is odd\n        k = int(ksize[0])\n        if k % 2 == 0: k += 1\n        kernel_size = [k, k] # (h, w)\n        \n        # If float64, convert to float32 for MPS (MPS doesn't support float64)\n        if arr.dtype == torch.float64:\n            arr = arr.to(torch.float32)\n            \n        if arr.ndim == 2:\n             # (H, W) -> unsqueeze -> (1, H, W)\n             img_tensor = arr.unsqueeze(0)\n             blurred = F.gaussian_blur(img_tensor, kernel_size, [sigma, sigma])\n             return blurred.squeeze(0)\n        else:\n            # (H, W, C) -> Permute -> (C, H, W)\n            img_tensor = arr.permute(2, 0, 1)\n            blurred = F.gaussian_blur(img_tensor, kernel_size, [sigma, sigma])\n            return blurred.permute(1, 2, 0)\n\n    else:\n        # NumPy/OpenCV\n        return cv2.GaussianBlur(arr, ksize, sigma)\n\ndef resize(arr, size, interpolation=cv2.INTER_LINEAR):\n    \"\"\"\n    Resize image.\n    size: (width, height)\n    \"\"\"\n    target_w, target_h = size\n    \n    if BACKEND_TYPE.startswith(\"CUDA (CuPy)\") and isinstance(arr, cp.ndarray):\n        h, w = arr.shape[:2]\n        zoom_h = target_h / h\n        zoom_w = target_w / w\n        if arr.ndim == 2:\n            zoom_factors = (zoom_h, zoom_w)\n        else:\n            zoom_factors = (zoom_h, zoom_w, 1)\n        \n        order = 1\n        if interpolation == cv2.INTER_NEAREST: order = 0\n        elif interpolation in [cv2.INTER_CUBIC, cv2.INTER_LANCZOS4]: order = 3\n        \n        return cupyx.scipy.ndimage.zoom(arr, zoom_factors, order=order)\n        \n    elif \"PyTorch\" in BACKEND_TYPE and isinstance(arr, torch.Tensor):\n        import torch.nn.functional as F\n        \n        # F.interpolate expects (N, C, H, W)\n        # arr is (H, W) or (H, W, C)\n        \n        if arr.ndim == 2:\n            # (H, W) -> (1, 1, H, W)\n            x = arr.unsqueeze(0).unsqueeze(0)\n        elif arr.ndim == 3:\n            # (H, W, C) -> (1, C, H, W)\n            x = arr.permute(2, 0, 1).unsqueeze(0)\n            \n        mode = 'bilinear'\n        if interpolation == cv2.INTER_NEAREST: mode = 'nearest'\n        elif interpolation in [cv2.INTER_CUBIC, cv2.INTER_LANCZOS4]: mode = 'bicubic'\n        \n        align_corners = False \n        # For nearest, align_corners must be None\n        if mode == 'nearest': align_corners = None\n        \n        # Cast to float32 for interpolation (some MPS ops don't support uint8)\n        original_dtype = arr.dtype\n        x = x.to(torch.float32)\n        \n        out = F.interpolate(x, size=(target_h, target_w), mode=mode, align_corners=align_corners)\n        \n        # Cast back if necessary (e.g. for image display)\n        # But wait, core logic usually expects float (0-1) for processing, but standardize takes uint8 inputs?\n        # standardize takes whatever comes in.\n        # If output needs to be uint8, cast back.\n        if original_dtype == torch.uint8:\n             out = out.clamp(0, 255).to(torch.uint8)\n        \n        if arr.ndim == 2:\n            return out.squeeze(0).squeeze(0)\n        elif arr.ndim == 3:\n            return out.squeeze(0).permute(1, 2, 0)\n\n    else:\n        return cv2.resize(arr, size, interpolation=interpolation)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%%writefile core/presets.py\ndef film_choose(film_type):\n    if film_type == (\"NC200\"):\n        r_r = 0.77 #\u7ea2\u8272\u611f\u5149\u5c42\u5438\u6536\u7684\u7ea2\u5149\n        r_g = 0.12 #\u7ea2\u8272\u611f\u5149\u5c42\u5438\u6536\u7684\u7eff\u5149\n        r_b = 0.18 #\u7ea2\u8272\u611f\u5149\u5c42\u5438\u6536\u7684\u84dd\u5149\n        g_r = 0.08 #\u7eff\u8272\u611f\u5149\u5c42\u5438\u6536\u7684\u7ea2\u5149\n        g_g = 0.85 #\u7eff\u8272\u611f\u5149\u5c42\u5438\u6536\u7684\u7eff\u5149\n        g_b = 0.23 #\u7eff\u8272\u611f\u5149\u5c42\u5438\u6536\u7684\u84dd\u5149\n        b_r = 0.08 #\u84dd\u8272\u611f\u5149\u5c42\u5438\u6536\u7684\u7ea2\u5149\n        b_g = 0.09 #\u84dd\u8272\u611f\u5149\u5c42\u5438\u6536\u7684\u7eff\u5149\n        b_b = 0.92 #\u84dd\u8272\u611f\u5149\u5c42\u5438\u6536\u7684\u84dd\u5149\n        t_r = 0.25 #\u5168\u8272\u611f\u5149\u5c42\u5438\u6536\u7684\u7ea2\u5149\n        t_g = 0.35 #\u5168\u8272\u611f\u5149\u5c42\u5438\u6536\u7684\u7eff\u5149\n        t_b = 0.35 #\u5168\u8272\u611f\u5149\u5c42\u5438\u6536\u7684\u84dd\u5149\n        color_type = (\"color\") #\u8272\u5f69\u7c7b\u578b\n        sens_factor = 1.20 #\u9ad8\u5149\u654f\u611f\u7cfb\u6570\n        d_r = 1.48 #\u7ea2\u8272\u611f\u5149\u5c42\u63a5\u53d7\u7684\u6563\u5c04\u5149\n        l_r = 0.95 #\u7ea2\u8272\u611f\u5149\u5c42\u63a5\u53d7\u7684\u76f4\u5c04\u5149\n        x_r = 1.18 #\u7ea2\u8272\u611f\u5149\u5c42\u7684\u54cd\u5e94\u7cfb\u6570\n        n_r = 0.18 #\u7ea2\u8272\u611f\u5149\u5c42\u7684\u9897\u7c92\u5ea6\n        d_g = 1.02 #\u7eff\u8272\u611f\u5149\u5c42\u63a5\u53d7\u7684\u6563\u5c04\u5149\n        l_g = 0.80 #\u7eff\u8272\u611f\u5149\u5c42\u63a5\u53d7\u7684\u76f4\u5c04\u5149\n        x_g = 1.02 #\u7eff\u8272\u611f\u5149\u5c42\u7684\u54cd\u5e94\u7cfb\u6570\n        n_g = 0.18 #\u7eff\u8272\u611f\u5149\u5c42\u7684\u9897\u7c92\u5ea6\n        d_b = 1.02 #\u84dd\u8272\u611f\u5149\u5c42\u63a5\u53d7\u7684\u6563\u5c04\u5149\n        l_b = 0.88 #\u84dd\u8272\u611f\u5149\u5c42\u63a5\u53d7\u7684\u76f4\u5c04\u5149\n        x_b = 0.78 #\u84dd\u8272\u611f\u5149\u5c42\u7684\u54cd\u5e94\u7cfb\u6570\n        n_b = 0.18 #\u84dd\u8272\u611f\u5149\u5c42\u7684\u9897\u7c92\u5ea6\n        d_l = None #\u5168\u8272\u611f\u5149\u5c42\u63a5\u53d7\u7684\u6563\u5c04\u5149\n        l_l = None #\u5168\u8272\u611f\u5149\u5c42\u63a5\u53d7\u7684\u76f4\u5c04\u5149\n        x_l = None #\u5168\u8272\u611f\u5149\u5c42\u7684\u54cd\u5e94\u7cfb\u6570\n        n_l = 0.08 #\u5168\u8272\u611f\u5149\u5c42\u7684\u9897\u7c92\u5ea6\n        gamma = 2.05\n        A = 0.15 #\u80a9\u90e8\u5f3a\u5ea6\n        B = 0.50 #\u7ebf\u6027\u6bb5\u5f3a\u5ea6\n        C = 0.10 #\u7ebf\u6027\u6bb5\u5e73\u6574\u5ea6\n        D = 0.20 #\u8dbe\u90e8\u5f3a\u5ea6\n        E = 0.02 #\u8dbe\u90e8\u786c\u5ea6\n        F = 0.30 #\u8dbe\u90e8\u8f6f\u5ea6\n    elif film_type == (\"FS200\"):\n        r_r = 0 #\u7ea2\u8272\u611f\u5149\u5c42\u5438\u6536\u7684\u7ea2\u5149\n        r_g = 0 #\u7ea2\u8272\u611f\u5149\u5c42\u5438\u6536\u7684\u7eff\u5149\n        r_b = 0 #\u7ea2\u8272\u611f\u5149\u5c42\u5438\u6536\u7684\u84dd\u5149\n        g_r = 0 #\u7eff\u8272\u611f\u5149\u5c42\u5438\u6536\u7684\u7ea2\u5149\n        g_g = 0 #\u7eff\u8272\u611f\u5149\u5c42\u5438\u6536\u7684\u7eff\u5149\n        g_b = 0 #\u7eff\u8272\u611f\u5149\u5c42\u5438\u6536\u7684\u84dd\u5149\n        b_r = 0 #\u84dd\u8272\u611f\u5149\u5c42\u5438\u6536\u7684\u7ea2\u5149\n        b_g = 0 #\u84dd\u8272\u611f\u5149\u5c42\u5438\u6536\u7684\u7eff\u5149\n        b_b = 0 #\u84dd\u8272\u611f\u5149\u5c42\u5438\u6536\u7684\u84dd\u5149\n        t_r = 0.15 #\u5168\u8272\u611f\u5149\u5c42\u5438\u6536\u7684\u7ea2\u5149\n        t_g = 0.35 #\u5168\u8272\u611f\u5149\u5c42\u5438\u6536\u7684\u7eff\u5149\n        t_b = 0.45 #\u5168\u8272\u611f\u5149\u5c42\u5438\u6536\u7684\u84dd\u5149\n        color_type = (\"single\") #\u8272\u5f69\u7c7b\u578b\n        sens_factor = 1.0 #\u9ad8\u5149\u654f\u611f\u7cfb\u6570\n        d_r = 0 #\u7ea2\u8272\u611f\u5149\u5c42\u63a5\u53d7\u7684\u6563\u5c04\u5149\n        l_r = 0 #\u7ea2\u8272\u611f\u5149\u5c42\u63a5\u53d7\u7684\u76f4\u5c04\u5149\n        x_r = 0 #\u7ea2\u8272\u611f\u5149\u5c42\u7684\u54cd\u5e94\u7cfb\u6570\n        n_r = 0 #\u7ea2\u8272\u611f\u5149\u5c42\u7684\u9897\u7c92\u5ea6\n        d_g = 0 #\u7eff\u8272\u611f\u5149\u5c42\u63a5\u53d7\u7684\u6563\u5c04\u5149\n        l_g = 0 #\u7eff\u8272\u611f\u5149\u5c42\u63a5\u53d7\u7684\u76f4\u5c04\u5149\n        x_g = 0 #\u7eff\u8272\u611f\u5149\u5c42\u7684\u54cd\u5e94\u7cfb\u6570\n        n_g = 0 #\u7eff\u8272\u611f\u5149\u5c42\u7684\u9897\u7c92\u5ea6\n        d_b = 0 #\u84dd\u8272\u611f\u5149\u5c42\u63a5\u53d7\u7684\u6563\u5c04\u5149\n        l_b = 0 #\u84dd\u8272\u611f\u5149\u5c42\u63a5\u53d7\u7684\u76f4\u5c04\u5149\n        x_b = 0 #\u84dd\u8272\u611f\u5149\u5c42\u7684\u54cd\u5e94\u7cfb\u6570\n        n_b = 0 #\u84dd\u8272\u611f\u5149\u5c42\u7684\u9897\u7c92\u5ea6\n        d_l = 2.33 #\u5168\u8272\u611f\u5149\u5c42\u63a5\u53d7\u7684\u6563\u5c04\u5149\n        l_l = 0.85 #\u5168\u8272\u611f\u5149\u5c42\u63a5\u53d7\u7684\u76f4\u5c04\u5149\n        x_l = 1.15 #\u5168\u8272\u611f\u5149\u5c42\u7684\u54cd\u5e94\u7cfb\u6570\n        n_l = 0.20 #\u5168\u8272\u611f\u5149\u5c42\u7684\u9897\u7c92\u5ea6\n        gamma = 2.2\n        A = 0.15 #\u80a9\u90e8\u5f3a\u5ea6\n        B = 0.50 #\u7ebf\u6027\u6bb5\u5f3a\u5ea6\n        C = 0.10 #\u7ebf\u6027\u6bb5\u5e73\u6574\u5ea6\n        D = 0.20 #\u8dbe\u90e8\u5f3a\u5ea6\n        E = 0.02 #\u8dbe\u90e8\u786c\u5ea6\n        F = 0.30 #\u8dbe\u90e8\u8f6f\u5ea6\n    elif film_type == (\"AS100\"):\n        r_r = 0 #\u7ea2\u8272\u611f\u5149\u5c42\u5438\u6536\u7684\u7ea2\u5149\n        r_g = 0 #\u7ea2\u8272\u611f\u5149\u5c42\u5438\u6536\u7684\u7eff\u5149\n        r_b = 0 #\u7ea2\u8272\u611f\u5149\u5c42\u5438\u6536\u7684\u84dd\u5149\n        g_r = 0 #\u7eff\u8272\u611f\u5149\u5c42\u5438\u6536\u7684\u7ea2\u5149\n        g_g = 0 #\u7eff\u8272\u611f\u5149\u5c42\u5438\u6536\u7684\u7eff\u5149\n        g_b = 0 #\u7eff\u8272\u611f\u5149\u5c42\u5438\u6536\u7684\u84dd\u5149\n        b_r = 0 #\u84dd\u8272\u611f\u5149\u5c42\u5438\u6536\u7684\u7ea2\u5149\n        b_g = 0 #\u84dd\u8272\u611f\u5149\u5c42\u5438\u6536\u7684\u7eff\u5149\n        b_b = 0 #\u84dd\u8272\u611f\u5149\u5c42\u5438\u6536\u7684\u84dd\u5149\n        t_r = 0.30 #\u5168\u8272\u611f\u5149\u5c42\u5438\u6536\u7684\u7ea2\u5149\n        t_g = 0.12 #\u5168\u8272\u611f\u5149\u5c42\u5438\u6536\u7684\u7eff\u5149\n        t_b = 0.45 #\u5168\u8272\u611f\u5149\u5c42\u5438\u6536\u7684\u84dd\u5149\n        color_type = (\"single\") #\u8272\u5f69\u7c7b\u578b\n        sens_factor = 1.28 #\u9ad8\u5149\u654f\u611f\u7cfb\u6570\n        d_r = 0 #\u7ea2\u8272\u611f\u5149\u5c42\u63a5\u53d7\u7684\u6563\u5c04\u5149\n        l_r = 0 #\u7ea2\u8272\u611f\u5149\u5c42\u63a5\u53d7\u7684\u76f4\u5c04\u5149\n        x_r = 0 #\u7ea2\u8272\u611f\u5149\u5c42\u7684\u54cd\u5e94\u7cfb\u6570\n        n_r = 0 #\u7ea2\u8272\u611f\u5149\u5c42\u7684\u9897\u7c92\u5ea6\n        d_g = 0 #\u7eff\u8272\u611f\u5149\u5c42\u63a5\u53d7\u7684\u6563\u5c04\u5149\n        l_g = 0 #\u7eff\u8272\u611f\u5149\u5c42\u63a5\u53d7\u7684\u76f4\u5c04\u5149\n        x_g = 0 #\u7eff\u8272\u611f\u5149\u5c42\u7684\u54cd\u5e94\u7cfb\u6570\n        n_g = 0 #\u7eff\u8272\u611f\u5149\u5c42\u7684\u9897\u7c92\u5ea6\n        d_b = 0 #\u84dd\u8272\u611f\u5149\u5c42\u63a5\u53d7\u7684\u6563\u5c04\u5149\n        l_b = 0 #\u84dd\u8272\u611f\u5149\u5c42\u63a5\u53d7\u7684\u76f4\u5c04\u5149\n        x_b = 0 #\u84dd\u8272\u611f\u5149\u5c42\u7684\u54cd\u5e94\u7cfb\u6570\n        n_b = 0 #\u84dd\u8272\u611f\u5149\u5c42\u7684\u9897\u7c92\u5ea6\n        d_l = 1.0 #\u5168\u8272\u611f\u5149\u5c42\u63a5\u53d7\u7684\u6563\u5c04\u5149\n        l_l = 1.05 #\u5168\u8272\u611f\u5149\u5c42\u63a5\u53d7\u7684\u76f4\u5c04\u5149\n        x_l = 1.25 #\u5168\u8272\u611f\u5149\u5c42\u7684\u54cd\u5e94\u7cfb\u6570\n        n_l = 0.10 #\u5168\u8272\u611f\u5149\u5c42\u7684\u9897\u7c92\u5ea6\n        gamma = 2.0\n        A = 0.15 #\u80a9\u90e8\u5f3a\u5ea6\n        B = 0.50 #\u7ebf\u6027\u6bb5\u5f3a\u5ea6\n        C = 0.25 #\u7ebf\u6027\u6bb5\u5e73\u6574\u5ea6\n        D = 0.35 #\u8dbe\u90e8\u5f3a\u5ea6\n        E = 0.02 #\u8dbe\u90e8\u786c\u5ea6\n        F = 0.35 #\u8dbe\u90e8\u8f6f\u5ea6\n        \n    return r_r,r_g,r_b,g_r,g_g,g_b,b_r,b_g,b_b,t_r,t_g,t_b,color_type,sens_factor,d_r,l_r,x_r,n_r,d_g,l_g,x_g,n_g,d_b,l_b,x_b,n_b,d_l,l_l,x_l,n_l,gamma,A,B,C,D,E,F\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%%writefile core/processing.py\nimport cv2\nimport time\nfrom .presets import film_choose\nfrom .backend import xp, to_cpu, to_gpu, gaussian_blur, resize\n\ndef standardize(image, min_size=3000):\n    \"\"\"\u6807\u51c6\u5316\u56fe\u50cf\u5c3a\u5bf8\"\"\"\n    \n    # If min_size is <= 0, do not resize (keep original)\n    if min_size <= 0:\n        return image\n\n    # \u83b7\u53d6\u539f\u59cb\u5c3a\u5bf8\n    # Ensure image size calculation works on both\n    height, width = image.shape[:2]\n    \n    # If image is already smaller than min_size, maybe don't upscale? \n    # Original logic always resized based on short edge being min_size (so it upscales small images too).\n    # We keep original logic for consistency unless min_size is passed.\n    \n    # \u786e\u5b9a\u7f29\u653e\u6bd4\u4f8b\n    if height < width:\n        # \u7ad6\u56fe - \u9ad8\u5ea6\u4e3a\u77ed\u8fb9\n        scale_factor = min_size / height\n        new_height = min_size\n        new_width = int(width * scale_factor)\n    else:\n        # \u6a2a\u56fe - \u5bbd\u5ea6\u4e3a\u77ed\u8fb9\n        scale_factor = min_size / width\n        new_width = min_size\n        new_height = int(height * scale_factor)\n    \n    # \u786e\u4fdd\u65b0\u5c3a\u5bf8\u4e3a\u5076\u6570\uff08\u907f\u514d\u67d0\u4e9b\u5904\u7406\u95ee\u9898\uff09\n    new_width = new_width + 1 if new_width % 2 != 0 else new_width\n    new_height = new_height + 1 if new_height % 2 != 0 else new_height\n    \n    interpolation = cv2.INTER_AREA if scale_factor < 1 else cv2.INTER_LANCZOS4\n    \n    # Use backend resize\n    resized_image = resize(image, (new_width, new_height), interpolation=interpolation)\n\n    return resized_image\n    #\u7edf\u4e00\u5c3a\u5bf8\n\n# ... (luminance, average, grain, reinhard, filmic, opt unchanged) ...\n\ndef process(image, film_type, grain_style, Tone_style, resolution=3000):\n    \n    start_time = time.time()\n    \n    # Upload to GPU if available\n    image = to_gpu(image)\n\n    # \u83b7\u53d6\u80f6\u7247\u53c2\u6570\n    (r_r,r_g,r_b,g_r,g_g,g_b,b_r,b_g,b_b,t_r,t_g,t_b,color_type,sens_factor,d_r,l_r,x_r,n_r,d_g,l_g,x_g,n_g,d_b,l_b,x_b,n_b,d_l,l_l,x_l,n_l,gamma,A,B,C,D,E,F) = film_choose(film_type)\n    \n    if grain_style == (\"\u9ed8\u8ba4\"):\n        n_r = n_r * 1.0\n        n_g = n_g * 1.0\n        n_b = n_b * 1.0\n        n_l = n_l * 1.0\n    elif grain_style == (\"\u67d4\u548c\"):\n        n_r = n_r * 0.5\n        n_g = n_g * 0.5\n        n_b = n_b * 0.5\n        n_l = n_l * 0.5\n    elif grain_style == (\"\u8f83\u7c97\"):\n        n_r = n_r * 1.5\n        n_g = n_g * 1.5\n        n_b = n_b * 1.5\n        n_l = n_l * 1.5\n    elif grain_style == (\"\u4e0d\u4f7f\u7528\"):\n        n_r = n_r * 0\n        n_g = n_g * 0\n        n_b = n_b * 0\n        n_l = n_l * 0\n\n\n    # \u8c03\u6574\u5c3a\u5bf8\n    image = standardize(image, min_size=resolution)\n\n    (lux_r,lux_g,lux_b,lux_total) = luminance(image,color_type,r_r,r_g,r_b,g_r,g_g,g_b,b_r,b_g,b_b,t_r,t_g,t_b)\n    #\u91cd\u5efa\u5149\u7ebf\n    film = opt(lux_r,lux_g,lux_b,lux_total,color_type, sens_factor, d_r, l_r, x_r, n_r, d_g, l_g, x_g, n_g, d_b, l_b, x_b, n_b, d_l, l_l, x_l, n_l,grain_style,gamma,A,B,C,D,E,F,Tone_style)\n    #\u51b2\u6d17\u5e95\u7247\n    \n    # Download from GPU to CPU (ensure result is numpy)\n    film = to_cpu(film)\n\n    process_time = time.time() - start_time\n\n    return film, process_time\n    #\u7edf\u4e00\u5c3a\u5bf8\n\ndef luminance(image,color_type,r_r,r_g,r_b,g_r,g_g,g_b,b_r,b_g,b_b,t_r,t_g,t_b):\n    \"\"\"\u8ba1\u7b97\u4eae\u5ea6\u56fe\u50cf (0-1\u8303\u56f4)\"\"\"\n    # \u5206\u79bbRGB\u901a\u9053 - Replace cv2.split with slicing\n    b = image[:, :, 0]\n    g = image[:, :, 1]\n    r = image[:, :, 2]\n    \n    # \u8f6c\u6362\u4e3a\u6d6e\u70b9\u6570\n    b_float = xp.astype(b, xp.float32) / 255.0\n    g_float = xp.astype(g, xp.float32) / 255.0\n    r_float = xp.astype(r, xp.float32) / 255.0\n    \n    # \u6a21\u62df\u4e0d\u540c\u4e73\u5242\u5c42\u7684\u5438\u6536\u7279\u6027\n    if color_type == (\"color\"):\n        lux_r = r_r * r_float + r_g * g_float + r_b * b_float\n        lux_g = g_r * r_float + g_g * g_float + g_b * b_float\n        lux_b = b_r * r_float + b_g * g_float + b_b * b_float\n        lux_total = t_r * r_float + t_g * g_float + t_b * b_float\n    else:\n        lux_total = t_r * r_float + t_g * g_float + t_b * b_float\n        lux_r = None\n        lux_g = None\n        lux_b = None\n\n    return lux_r,lux_g,lux_b,lux_total\n    #\u5b9e\u73b0\u5bf9\u6e90\u56fe\u50cf\u7684\u5206\u5149\u5e76\u6574\u5408\u8f93\u51fa\n\ndef average(lux_total):\n    \"\"\"\u8ba1\u7b97\u56fe\u50cf\u7684\u5e73\u5747\u4eae\u5ea6 (0-1)\"\"\"\n    # \u8ba1\u7b97\u5e73\u5747\u4eae\u5ea6\n    avg_lux = xp.mean(lux_total)\n    avg_lux = xp.clip(avg_lux,0,1)\n    return avg_lux\n    #\u8ba1\u7b97\u5e73\u5747\u4eae\u5ea6\n\ndef grain(lux_r,lux_g,lux_b,lux_total,color_type,sens):\n    #\u57fa\u4e8e\u52a0\u6743\u968f\u673a\u7684\u9897\u7c92\u6a21\u62df\n    if color_type == (\"color\"):\n\n        # \u521b\u5efa\u6b63\u8d1f\u566a\u58f0\n        noise = xp.astype(xp.random.normal(0,1, lux_r.shape), xp.float32)\n        noise = noise ** 2\n        noise = noise * (xp.random.choice([-1, 1],lux_r.shape))\n        # \u521b\u5efa\u6743\u91cd\u56fe (\u4e2d\u7b49\u4eae\u5ea6\u533a\u57df\u6743\u91cd\u6700\u9ad8)\n        weights =(0.5 - xp.abs(lux_r - 0.5)) * 2\n        weights = xp.clip(weights,0.05,0.9)\n        # \u5e94\u7528\u6743\u91cd\n        sens_grain = xp.clip (sens,0.4,0.6)\n        weighted_noise = noise * weights* sens_grain\n        # \u6dfb\u52a0\u8f7b\u5fae\u6a21\u7cca\n        # Use backend gaussian_blur\n        weighted_noise = gaussian_blur(weighted_noise, (3, 3), 1)\n        weighted_noise_r = xp.clip(weighted_noise, -1,1)\n        # \u5e94\u7528\u9897\u7c92\n\n        # \u521b\u5efa\u6b63\u8d1f\u566a\u58f0\n        noise = xp.astype(xp.random.normal(0,1, lux_g.shape), xp.float32)\n        noise = noise ** 2\n        noise = noise * (xp.random.choice([-1, 1],lux_g.shape))\n        # \u521b\u5efa\u6743\u91cd\u56fe (\u4e2d\u7b49\u4eae\u5ea6\u533a\u57df\u6743\u91cd\u6700\u9ad8)\n        weights =(0.5 - xp.abs(lux_g - 0.5)) * 2\n        weights = xp.clip(weights,0.05,0.9)\n        # \u5e94\u7528\u6743\u91cd\n        sens_grain = xp.clip (sens,0.4,0.6)\n        weighted_noise = noise * weights* sens_grain\n        # \u6dfb\u52a0\u8f7b\u5fae\u6a21\u7cca\n        weighted_noise = gaussian_blur(weighted_noise, (3, 3), 1)\n        weighted_noise_g = xp.clip(weighted_noise, -1,1)\n        # \u5e94\u7528\u9897\u7c92\n\n        # \u521b\u5efa\u6b63\u8d1f\u566a\u58f0\n        noise = xp.astype(xp.random.normal(0,1, lux_b.shape), xp.float32)\n        noise = noise ** 2\n        noise = noise * (xp.random.choice([-1, 1],lux_b.shape))\n        # \u521b\u5efa\u6743\u91cd\u56fe (\u4e2d\u7b49\u4eae\u5ea6\u533a\u57df\u6743\u91cd\u6700\u9ad8)\n        weights =(0.5 - xp.abs(lux_b - 0.5)) * 2\n        weights = xp.clip(weights,0.05,0.9)\n        # \u5e94\u7528\u6743\u91cd\n        sens_grain = xp.clip (sens,0.4,0.6)\n        weighted_noise = noise * weights* sens_grain\n        # \u6dfb\u52a0\u8f7b\u5fae\u6a21\u7cca\n        weighted_noise = gaussian_blur(weighted_noise, (3, 3), 1)\n        weighted_noise_b = xp.clip(weighted_noise, -1,1)\n        \n        weighted_noise_total = None\n        # \u5e94\u7528\u9897\u7c92\n        \n    else:\n\n        # \u521b\u5efa\u6b63\u8d1f\u566a\u58f0\n        noise = xp.astype(xp.random.normal(0,1, lux_total.shape), xp.float32)\n        noise = noise ** 2\n        noise = noise * (xp.random.choice([-1, 1],lux_total.shape))\n        # \u521b\u5efa\u6743\u91cd\u56fe (\u4e2d\u7b49\u4eae\u5ea6\u533a\u57df\u6743\u91cd\u6700\u9ad8)\n        weights =(0.5 - xp.abs(lux_total - 0.5)) * 2\n        weights = xp.clip(weights,0.05,0.9)\n        # \u5e94\u7528\u6743\u91cd\n        sens_grain = xp.clip (sens,0.4,0.6)\n        weighted_noise = noise * weights* sens_grain\n        # \u6dfb\u52a0\u8f7b\u5fae\u6a21\u7cca\n        weighted_noise = gaussian_blur(weighted_noise, (3, 3), 1)\n        weighted_noise_total = xp.clip(weighted_noise, -1,1)\n        weighted_noise_r = None\n        weighted_noise_g = None\n        weighted_noise_b = None\n        # \u5e94\u7528\u9897\u7c92\n    \n    return weighted_noise_r,weighted_noise_g,weighted_noise_b,weighted_noise_total\n    #\u521b\u5efa\u9897\u7c92\u51fd\u6570\n\ndef reinhard(lux_r,lux_g,lux_b,lux_total,color_type,gamma):\n    #\u5b9a\u4e49reinhard\u7b97\u6cd5\uff0cexp\u4e3a\u66dd\u5149\u5ea6\uff0cgam\u4e3a\u4f3d\u9a6c\u503c\n    \n    if color_type == \"color\":\n\n        mapped = lux_r\n        #\u5b9a\u4e49\u8f93\u5165\u7684\u56fe\u50cf\n        mapped = mapped * (mapped/ (1.0 + mapped))\n        #\u5e94\u7528reinhard\u7b97\u6cd5\n        mapped = xp.power(mapped, 1.05/gamma)\n        result_r = xp.clip(mapped,0,1)\n\n        mapped = lux_g\n        #\u5b9a\u4e49\u8f93\u5165\u7684\u56fe\u50cf\n        mapped = mapped * (mapped/ (1.0 + mapped))\n        #\u5e94\u7528reinhard\u7b97\u6cd5\n        mapped = xp.power(mapped, 1.05/gamma)\n        result_g = xp.clip(mapped,0,1)\n\n        mapped = lux_b\n        #\u5b9a\u4e49\u8f93\u5165\u7684\u56fe\u50cf\n        mapped = mapped * (mapped/ (1.0 + mapped))\n        #\u5e94\u7528reinhard\u7b97\u6cd5\n        mapped = xp.power(mapped, 1.05/gamma)\n        result_b = xp.clip(mapped,0,1)\n        result_total = None\n    else:\n        mapped = lux_total\n        #\u5b9a\u4e49\u8f93\u5165\u7684\u56fe\u50cf\n        mapped = mapped * (mapped/ (1.0 + mapped))\n        #\u5e94\u7528reinhard\u7b97\u6cd5\n        mapped = xp.power(mapped, 1.0/gamma)\n        result_total = xp.clip(mapped,0,1)\n        result_r = None\n        result_g = None\n        result_b = None\n\n    return result_r,result_g,result_b,result_total\n    #\u521b\u5efareinhard\u51fd\u6570\n\ndef filmic(lux_r,lux_g,lux_b,lux_total,color_type,gamma,A,B,C,D,E,F):\n    #fimlic\u6620\u5c04\n\n    if color_type == (\"color\"):\n\n        lux_r = xp.maximum(lux_r, 0)\n        lux_g = xp.maximum(lux_g, 0)\n        lux_b = xp.maximum(lux_b, 0)\n\n        lux_r = 10 * (lux_r ** gamma)\n        lux_g = 10 * (lux_g ** gamma)\n        lux_b = 10 * (lux_b ** gamma)\n\n        result_r = ((lux_r * (A * lux_r + C * B) + D * E) / (lux_r * (A * lux_r + B) + D * F)) - E/F\n        result_g = ((lux_g * (A * lux_g + C * B) + D * E) / (lux_g * (A * lux_g + B) + D * F)) - E/F\n        result_b = ((lux_b * (A * lux_b + C * B) + D * E) / (lux_b * (A * lux_b + B) + D * F)) - E/F\n        result_total = None\n    else:\n        lux_total = xp.maximum(lux_total, 0)\n        lux_total = 10 * (lux_total ** gamma)\n        result_r = None\n        result_g = None\n        result_b = None\n        result_total = ((lux_total * (A * lux_total + C * B) + D * E) / (lux_total * (A * lux_total + B) + D * F)) - E/F\n    \n    return result_r,result_g,result_b,result_total\n\ndef opt(lux_r,lux_g,lux_b,lux_total,color_type, sens_factor, d_r, l_r, x_r, n_r, d_g, l_g, x_g, n_g, d_b, l_b, x_b, n_b, d_l, l_l, x_l, n_l,grain_style,gamma,A,B,C,D,E,F,Tone_style):\n    #opt \u5149\u5b66\u6269\u6563\u51fd\u6570\n\n    avrl = average(lux_total)\n    # \u6839\u636e\u5e73\u5747\u4eae\u5ea6\u8ba1\u7b97\u654f\u611f\u5ea6\n    sens = (1.0 - avrl) * 0.75 + 0.10\n    # \u5c06\u654f\u611f\u5ea6\u9650\u5236\u57280-1\u8303\u56f4\u5185\n    sens = xp.clip(sens,0.10,0.7) #sens -- \u9ad8\u5149\u654f\u611f\u5ea6\n    strg = 23 * sens**2 * sens_factor #strg -- \u5149\u6655\u5f3a\u5ea6\n\n    base = 0.05 * sens_factor #base -- \u57fa\u7840\u6269\u6563\u5f3a\u5ea6\n\n    # Downsampling factor for bloom optimization\n    bloom_scale = 0.125  # 1/8 size\n\n    # ksize not strictly needed for backend blur if we pass sigma, \n    # but we derived it from params so let's check\n    # In backend.gaussian_blur we handle sigma.\n    # The original logic used `rads` (radius) to determine kernel size for cv2.\n    # cv2.GaussianBlur(..., (ksize, ksize), 1) -> sigma=1 (fixed??)\n    # Wait, original code:\n    # weighted_noise = cv2.GaussianBlur(weighted_noise, (3, 3), 1) # Sigma=1\n    # bloom:\n    # bloom_layer_small = cv2.GaussianBlur(lux_small * weights_small, (0, 0), sigma_small)\n    # where sigma_small = sens * sigma_mult * bloom_scale\n    pass\n    \n    # Pre-calculate scaled dimensions\n    h_small = int(lux_total.shape[0] * bloom_scale)\n    w_small = int(lux_total.shape[1] * bloom_scale)\n    \n    # Helper to process bloom channel\n    def process_bloom_channel(lux_channel, sens, sigma_mult):\n        # Resize down\n        lux_small = resize(lux_channel, (w_small, h_small), interpolation=cv2.INTER_LINEAR)\n        \n        # Calculate weights on small image\n        weights_small = (base + lux_small**2) * sens\n        weights_small = xp.clip(weights_small, 0, 1)\n        \n        # Blur on small image\n        sigma_small = sens * sigma_mult * bloom_scale\n        \n        # Use backend blur. (0,0) logic handled by passing sigma directly.\n        bloom_layer_small = gaussian_blur(lux_small * weights_small, (0, 0), sigma_small)\n        \n        # Compute effect on small image\n        bloom_effect_small = bloom_layer_small * weights_small * strg\n        bloom_effect_small = (bloom_effect_small / (1.0 + bloom_effect_small))\n        \n        # Resize up\n        bloom_effect_large = resize(bloom_effect_small, (lux_channel.shape[1], lux_channel.shape[0]), interpolation=cv2.INTER_LINEAR)\n        return bloom_effect_large\n\n    if color_type == (\"color\"):\n        bloom_effect_r = process_bloom_channel(lux_r, sens, 55)\n        bloom_effect_g = process_bloom_channel(lux_g, sens, 35)\n        bloom_effect_b = process_bloom_channel(lux_b, sens, 15)\n        \n        if grain_style == (\"\u4e0d\u4f7f\u7528\"):\n            lux_r = bloom_effect_r * d_r + (lux_r**x_r) * l_r\n            lux_g = bloom_effect_g * d_g + (lux_g**x_g) * l_g\n            lux_b = bloom_effect_b * d_b + (lux_b**x_b) * l_b\n        else:    \n            (weighted_noise_r,weighted_noise_g,weighted_noise_b,weighted_noise_total) = grain(lux_r,lux_g,lux_b,lux_total,color_type,sens)\n            #\u5e94\u7528\u9897\u7c92\n            lux_r = bloom_effect_r * d_r + (lux_r**x_r) * l_r + weighted_noise_r *n_r + weighted_noise_g *n_l+ weighted_noise_b *n_l\n            lux_g = bloom_effect_g * d_g + (lux_g**x_g) * l_g + weighted_noise_r *n_l + weighted_noise_g *n_g+ weighted_noise_b *n_l\n            lux_b = bloom_effect_b * d_b + (lux_b**x_b) * l_b + weighted_noise_r *n_l + weighted_noise_g *n_l + weighted_noise_b *n_b\n        \n        #\u62fc\u5408\u5149\u5c42\n        if Tone_style == \"filmic\":\n            (result_r,result_g,result_b,result_total) = filmic(lux_r,lux_g,lux_b,lux_total,color_type,gamma,A,B,C,D,E,F)\n            #\u5e94\u7528flimic\u6620\u5c04\n        else:\n            (result_r,result_g,result_b,result_total) = reinhard(lux_r,lux_g,lux_b,lux_total,color_type,gamma)\n            #\u5e94\u7528\u6620\u5c04\n\n        combined_b = xp.astype((result_b * 255), xp.uint8)\n        combined_g = xp.astype((result_g * 255), xp.uint8)\n        combined_r = xp.astype((result_r * 255), xp.uint8)\n        # Use xp.dstack (depth stack) for merging\n        film = xp.dstack([combined_r, combined_g, combined_b])\n    else:\n        bloom_effect = process_bloom_channel(lux_total, sens, 55)\n\n        #\u5e94\u7528\u5149\u6655\n        if grain_style == (\"\u4e0d\u4f7f\u7528\"):\n            lux_total = bloom_effect * d_l + (lux_total**x_l) * l_l\n        else:\n            (weighted_noise_r,weighted_noise_g,weighted_noise_b,weighted_noise_total) = grain(lux_r,lux_g,lux_b,lux_total,color_type,sens)\n            #\u5e94\u7528\u9897\u7c92\n            lux_total = bloom_effect * d_l + (lux_total**x_l) * l_l + weighted_noise_total *n_l\n        \n        #\u62fc\u5408\u5149\u5c42\n        \n        if Tone_style == \"filmic\":\n            (result_r,result_g,result_b,result_total) = filmic(lux_r,lux_g,lux_b,lux_total,color_type,gamma,A,B,C,D,E,F)\n            #\u5e94\u7528flimic\u6620\u5c04\n        else:\n            (result_r,result_g,result_b,result_total) = reinhard(lux_r,lux_g,lux_b,lux_total,color_type,gamma)\n            #\u5e94\u7528reinhard\u6620\u5c04\n\n        film = xp.astype((result_total * 255), xp.uint8)\n\n    return film\n    #\u8fd4\u56de\u6e32\u67d3\u540e\u7684\u5149\u5ea6\n    #\u8fdb\u884c\u5e95\u7247\u6210\u50cf\n    #\u51c6\u5907\u6697\u623f\u5de5\u5177\n\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#@title 3. Upload File (Image or Video)\n", "\n", "uploaded = files.upload()\n", "input_filename = next(iter(uploaded))\n", "ext = os.path.splitext(input_filename)[1].lower()\n", "\n", "if ext in ['.jpg', '.jpeg', '.png', '.bmp']:\n", "    display(Image(input_filename, width=300))\n", "    file_mode = \"image\"\n", "elif ext in ['.mp4', '.avi', '.mov', '.mkv', '.m4v', '.webm', '.flv']:\n", "    print(f\"Video file uploaded: {input_filename}\")\n", "    file_mode = \"video\"\n", "else:\n", "    print(\"Unknown file type, converting as image...\")\n", "    file_mode = \"image\"\n", "\n", "print(f\"Uploaded: {input_filename} (Mode: {file_mode})\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#@title 4. Process\n", "#@markdown Select parameters. For video, resolution=0 keeps original size (recommended for video).\n", "\n", "film_type = \"NC200\" #@param [\"NC200\", \"AS100\", \"FS200\"]\n", "grain_style = \"\\u9ED8\\u8BA4\" #@param [\"\\u9ED8\\u8BA4\", \"\\u67D4\\u548C\", \"\\u8F83\\u7C97\", \"\\u4E0D\\u4F7F\\u7528\"]\n", "tone_style = \"reinhard\" #@param [\"filmic\", \"reinhard\"]\n", "target_resolution = 0 #@param {type:\"integer\", help:\"Short edge length. Set 0 to keep original size (recommended for video). Default 3000 for high-res photos.\"}\n", "\n", "from core.processing import process\n", "from core.backend import get_backend_name\n", "\n", "def process_and_display_image(filename):\n", "    img = cv2.imread(filename)\n", "    if img is None:\n", "        print(\"Error loading image\")\n", "        return\n", "        \n", "    print(f\"Processing Image with {get_backend_name()}...\")\n", "    res = target_resolution\n", "    if res <= 0:\n", "        res = -1 # Skip resize\n", "    else:\n", "        print(f\"Target resolution: {res} (short edge)\")\n", "        \n", "    film, p_time = process(img, film_type, grain_style, tone_style, resolution=res)\n", "    \n", "    print(f\"Done in {p_time:.2f}s\")\n", "    \n", "    output_filename = f\"processed_{film_type}_{int(time.time())}.jpg\"\n", "    film_bgr = cv2.cvtColor(film, cv2.COLOR_RGB2BGR)\n", "    cv2.imwrite(output_filename, film_bgr)\n", "    \n", "    display(Image(output_filename, width=500))\n", "    return output_filename\n", "\n", "def process_video(filename):\n", "    cap = cv2.VideoCapture(filename)\n", "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n", "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n", "    fps = cap.get(cv2.CAP_PROP_FPS)\n", "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n", "    \n", "    print(f\"Processing Video: {width}x{height} @ {fps}fps ({total_frames} frames)\")\n", "    print(f\"Device: {get_backend_name()}\")\n", "    \n", "    res = target_resolution\n", "    if res <= 0: \n", "        res = -1 # Keep original\n", "    else:\n", "        # Calculate target dimensions for writer\n", "        if height < width:\n", "            scale = res / height\n", "            new_h = res\n", "            new_w = int(width * scale)\n", "        else:\n", "            scale = res / width\n", "            new_w = res\n", "            new_h = int(height * scale)\n", "        width, height = new_w + (new_w%2), new_h + (new_h%2)\n", "        \n", "    timestamp = int(time.time())\n", "    base_name = f\"processed_{film_type}_{timestamp}\"\n", "    final_output_filename = f\"{base_name}.mp4\"\n", "    mute_filename = f\"{base_name}_mute.mp4\"\n", "    \n", "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n", "    out = cv2.VideoWriter(mute_filename, fourcc, fps, (width, height))\n", "    \n", "    try:\n", "        with tqdm(total=total_frames, unit='frame') as pbar:\n", "            while True:\n", "                ret, frame = cap.read()\n", "                if not ret: break\n", "                \n", "                film_rgb, _ = process(frame, film_type, grain_style, tone_style, resolution=res)\n", "                film_bgr = cv2.cvtColor(film_rgb, cv2.COLOR_RGB2BGR)\n", "                out.write(film_bgr)\n", "                pbar.update(1)\n", "    finally:\n", "        cap.release()\n", "        out.release()\n", "        \n", "    # Check for ffmpeg and merge audio\n", "    if shutil.which('ffmpeg') is not None:\n", "        print(\"Merging audio...\")\n", "        cmd = [\n", "            'ffmpeg', '-y',\n", "            '-i', mute_filename,\n", "            '-i', filename,\n", "            '-c:v', 'copy',\n", "            '-c:a', 'aac',\n", "            '-map', '0:v:0',\n", "            '-map', '1:a:0',\n", "            '-shortest',\n", "            final_output_filename\n", "        ]\n", "        try:\n", "            subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=True)\n", "            print(f\"Audio merged. Saved to {final_output_filename}\")\n", "            os.remove(mute_filename)\n", "            return final_output_filename\n", "        except subprocess.CalledProcessError as e:\n", "             print(f\"Audio merge failed. Returning mute video. Error: {e}\")\n", "             # Rename mute to final so user gets the expected file\n", "             if os.path.exists(mute_filename):\n", "                 os.rename(mute_filename, final_output_filename)\n", "             return final_output_filename\n", "    else:\n", "         print(\"ffmpeg not found. Returning mute video.\")\n", "         if os.path.exists(mute_filename):\n", "             os.rename(mute_filename, final_output_filename)\n", "         return final_output_filename\n", "\n", "if file_mode == \"image\":\n", "    output_file = process_and_display_image(input_filename)\n", "else:\n", "    output_file = process_video(input_filename)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#@title 5. Download Result\n", "if 'output_file' in locals():\n", "    files.download(output_file)\n", "else:\n", "    print(\"No output file generated yet.\")"]}], "metadata": {"colab": {"provenance": []}, "kernelspec": {"display_name": "Python 3", "name": "python3"}, "language_info": {"name": "python"}}, "nbformat": 4, "nbformat_minor": 0}