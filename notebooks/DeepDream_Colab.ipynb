{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepDream Revived - Colab\n",
    "\n",
    "Run the DeepDream algorithm on your images or videos directly in Google Colab.\n",
    "\n",
    "### Features\n",
    "- **Images & Videos**: Support for PNG, JPG, GIF, and MP4.\n",
    "- **Octaves Mode**: Multi-scale processing for high-detail dreams.\n",
    "- **Customizable**: Adjust steps, blend modes, and scales.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies\n",
    "Install necessary libraries and tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get install ffmpeg\n",
    "!pip install tensorflow numpy pillow\n",
    "\n",
    "import os\n",
    "print(\"Dependencies installed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Deploy Code\n",
    "Writing project files to Colab environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile dream.py\n",
    "import argparse\nimport random\nimport mimetypes\nimport os\nimport subprocess\nimport traceback\nimport sys\n\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\" # https://stackoverflow.com/questions/38073432/how-to-suppress-verbose-tensorflow-logging\nos.environ[\"AUTOGRAPH_VERBOSITY\"] = \"0\" # https://www.tensorflow.org/api_docs/python/tf/autograph/set_verbosity\nimport tensorflow\nimport numpy as np\nimport PIL.Image\nimport PIL.ImageChops\nfrom tensorflow.keras import layers, models\n\nffmpeg_path = None # Edit this if you want to specify a custom path to ffmpeg\nffprobe_path = ffmpeg_path # Usually you should not have to edit this\nffmpeg_exe = os.path.join(ffmpeg_path, 'ffmpeg') if ffmpeg_path else 'ffmpeg'\nffprobe_exe = os.path.join(ffprobe_path, 'ffprobe') if ffprobe_path else 'ffprobe'\n\n# Download an image and read it into a NumPy array.\ndef download(url, max_dim=None):\n    name = url.split('/')[-1]\n    image_path = tensorflow.keras.utils.get_file(name, origin=url)\n    img = PIL.Image.open(image_path)\n    if max_dim:\n        img.thumbnail((max_dim, max_dim))\n    return np.array(img)\n\ndef load_img(image_path : str, max_dim=None):\n    img = PIL.Image.open(image_path)\n    if max_dim:\n        img.thumbnail((max_dim, max_dim))\n    return np.array(img), img.size\n\ndef blend_img(next_image : str, prev_image : str, prev_dream_image : str, blend_amount=0.0, diff=False, max_dim=None):\n    img = PIL.Image.open(next_image).convert('RGB')\n    dream = PIL.Image.open(prev_dream_image).convert('RGB')\n    if diff:\n        prev = PIL.Image.open(prev_image).convert('RGB')\n        if prev.size != dream.size:\n            prev = prev.resize(dream.size)\n        diff = PIL.ImageChops.difference(dream,prev)\n        dream = diff\n    # Images need to be the same size (this can happen when the dream has been scaled up or down\n    if img.size != dream.size:\n        dream = dream.resize(img.size)\n\n    # Note: If blend is 0.0, a copy of image 1 is returned. If blend is 1.0, a copy of image 2 is returned.\n    # Therefore the previous dream image in the sequence should be passed in as image 2\n    blended = PIL.Image.blend(img, dream, blend_amount)\n    if max_dim:\n        blended.thumbnail((max_dim, max_dim))\n    return np.array(blended), blended.size\n\n# Normalize an image\ndef deprocess(img):\n    img = 255*(img + 1.0)/2.0\n    return tensorflow.cast(img, tensorflow.uint8)\n\ndef calc_loss(img, model):\n    # Pass forward the image through the model to retrieve the activations.\n    # Converts the image into a batch of size 1.\n    img_batch = tensorflow.expand_dims(img, axis=0)\n    layer_activations = model(img_batch)\n    if len(layer_activations) == 1:\n        layer_activations = [layer_activations]\n\n    losses = []\n    for act in layer_activations:\n        loss = tensorflow.math.reduce_mean(act)\n        losses.append(loss)\n\n    return  tensorflow.reduce_sum(losses)\n\nclass DeepDream(tensorflow.Module):\n    def __init__(self, model):\n        self.model = model\n\n    @tensorflow.function(\n        input_signature=(\n            tensorflow.TensorSpec(shape=[None,None,3], dtype=tensorflow.float32),\n            tensorflow.TensorSpec(shape=[], dtype=tensorflow.int32),\n            tensorflow.TensorSpec(shape=[], dtype=tensorflow.float32),)\n    )\n    def __call__(self, img, steps, step_size):\n        loss = tensorflow.constant(0.0)\n        for n in tensorflow.range(steps):\n            with tensorflow.GradientTape() as tape:\n                # This needs gradients relative to `img`\n                # `GradientTape` only watches `tensorflow.Variable`s by default\n                tape.watch(img)\n                loss = calc_loss(img, self.model)\n\n            # Calculate the gradient of the loss with respect to the pixels of the input image.\n            gradients = tape.gradient(loss, img)\n\n            # Normalize the gradients.\n            gradients /= tensorflow.math.reduce_std(gradients) + 1e-8 \n\n            # In gradient ascent, the \"loss\" is maximized so that the input image increasingly \"excites\" the layers.\n            # You can update the image by directly adding the gradients (because they're the same shape!)\n            img = img + gradients*step_size\n            img = tensorflow.clip_by_value(img, -1, 1)\n\n        return loss, img\n\ndef random_roll(img, maxroll):\n    # Randomly shift the image to avoid tiled boundaries.\n    shift = tensorflow.random.uniform(shape=[2], minval=-maxroll, maxval=maxroll, dtype=tensorflow.int32)\n    img_rolled = tensorflow.roll(img, shift=shift, axis=[0,1])\n    return shift, img_rolled\n\nclass TiledGradients(tensorflow.Module):\n    def __init__(self, model):\n        self.model = model\n\n    @tensorflow.function(\n        input_signature=(\n            tensorflow.TensorSpec(shape=[None,None,3], dtype=tensorflow.float32),\n            tensorflow.TensorSpec(shape=[2], dtype=tensorflow.int32),\n            tensorflow.TensorSpec(shape=[], dtype=tensorflow.int32),)\n    )\n    def __call__(self, img, img_size, tile_size=512):\n        shift, img_rolled = random_roll(img, tile_size)\n\n        # Initialize the image gradients to zero.\n        gradients = tensorflow.zeros_like(img_rolled)\n\n        # Skip the last tile, unless there's only one tile.\n        xs = tensorflow.range(0, img_size[1], tile_size)[:-1]\n        if not tensorflow.cast(len(xs), bool):\n            xs = tensorflow.constant([0])\n        ys = tensorflow.range(0, img_size[0], tile_size)[:-1]\n        if not tensorflow.cast(len(ys), bool):\n            ys = tensorflow.constant([0])\n\n        for x in xs:\n            for y in ys:\n                # Calculate the gradients for this tile.\n                with tensorflow.GradientTape() as tape:\n                    # This needs gradients relative to `img_rolled`.\n                    # `GradientTape` only watches `tensorflow.Variable`s by default.\n                    tape.watch(img_rolled)\n\n                    # Extract a tile out of the image.\n                    img_tile = img_rolled[y:y+tile_size, x:x+tile_size]\n                    loss = calc_loss(img_tile, self.model)\n\n                # Update the image gradients for this tile.\n                gradients = gradients + tape.gradient(loss, img_rolled)\n\n        # Undo the random shift applied to the image and its gradients.\n        gradients = tensorflow.roll(gradients, shift=-shift, axis=[0,1])\n\n        # Normalize the gradients.\n        gradients /= tensorflow.math.reduce_std(gradients) + 1e-8 \n        return gradients\n\n\ndef run_deep_dream_simple(img, dream_model, steps=100, step_size=0.01):\n    # Convert from uint8 to the range expected by the model.\n    img = tensorflow.keras.applications.inception_v3.preprocess_input(img)\n    img = tensorflow.convert_to_tensor(img)\n    step_size = tensorflow.convert_to_tensor(step_size)\n    steps_remaining = steps\n    step = 0\n    \n    # Check if we have cached the compiled function on the model\n    if not hasattr(dream_model, 'deep_dream_func'):\n        dream_model.deep_dream_func = DeepDream(dream_model)\n\n    while steps_remaining:\n        if steps_remaining>100:\n            run_steps = tensorflow.constant(100)\n        else:\n            run_steps = tensorflow.constant(steps_remaining)\n        steps_remaining -= run_steps\n        step += run_steps\n\n        loss, img = dream_model.deep_dream_func(img, run_steps, tensorflow.constant(step_size))\n        \n        display_img = deprocess(img)\n        # print('Step {}, loss {}'.format(step, loss))\n\n    result = deprocess(img)\n    return result\n\ndef run_deep_dream_with_octaves(img, dream_model, steps_per_octave=100, step_size=0.01, \n                                octaves=None, octave_scale=1.3, tile_size=512):\n    if octaves is None:\n        octaves = range(-2,3)\n    \n    # If tile_size is 0 or None, set to max(512, max_dim) to avoid tiling if possible, or just huge number\n    # If 0, user means auto/max.\n    if not tile_size or tile_size <= 0:\n        tile_size = max(img.shape[:2]) # Use full image size as one tile\n        \n    base_shape = tensorflow.cast(tensorflow.shape(img)[:-1], tensorflow.float32)\n    img = tensorflow.keras.applications.inception_v3.preprocess_input(img)\n\n    initial_shape = img.shape[:-1]\n    img = tensorflow.image.resize(img, initial_shape)\n    \n    # Prepare compiled functions\n    if not hasattr(dream_model, 'deep_dream_func'):\n        dream_model.deep_dream_func = DeepDream(dream_model)\n    if not hasattr(dream_model, 'tiled_gradients_func'):\n        dream_model.tiled_gradients_func = TiledGradients(dream_model)\n\n    for octave in octaves:\n        # Scale the image based on the octave\n        new_size = tensorflow.cast(base_shape * (octave_scale**octave), tensorflow.int32)\n        img = tensorflow.image.resize(img, new_size)\n\n        if tile_size is not None and (img.shape[0] > tile_size or img.shape[1] > tile_size):\n             # Use TiledGradients if image is larger than tile_size\n             # TiledGradients handles arbitrary size via tiling\n             for step in range(steps_per_octave):\n                 gradients = dream_model.tiled_gradients_func(img, new_size, tile_size=tile_size)\n                 img = img + gradients*step_size\n                 img = tensorflow.clip_by_value(img, -1, 1)\n        else:\n             # Use simple DeepDream if image fits in tile or no tile_size\n             loss, img = dream_model.deep_dream_func(img, steps_per_octave, step_size)\n\n    img = tensorflow.image.resize(img, initial_shape)\n    result = deprocess(img)\n    return result\n\ndef save_img(img, img_name : str):\n    PIL.Image.fromarray(np.array(img)).save(img_name, 'PNG')\n\ndef get_input_framerate(input_filename : str):\n    result = subprocess.run([ffprobe_exe,\"-v\", \"error\", \"-select_streams\", \"v\", \"-of\", \"default=noprint_wrappers=1:nokey=1\", \"-show_entries\", \"stream=r_frame_rate\", input_filename], stdout=subprocess.PIPE, text=True)\n    if result.returncode != 0:\n        print(result.stdout)\n        print('ffprobe returned error code {}'.format(result.returncode))\n        print('Error getting input fps.')\n        return None\n    # Outputs the frame rate as a precise fraction. Have to convert to decimal.\n    source_fps_fractional = result.stdout.split('/')\n    source_fps = round(float(source_fps_fractional[0]) / float(source_fps_fractional[1]), 2)\n    return source_fps\n\ndef output_to_png_sequence(input_filename : str, output_dir : str):\n    ffmpeg_args = [\"ffmpeg\", '-hide_banner', '-y', '-i', input_filename]\n    framerate = get_input_framerate(input_filename)\n    if framerate is not None:\n        ffmpeg_args.extend(['-vf', f'fps={framerate}'])\n    ffmpeg_args.append(os.path.join(output_dir,'%03d.png'))\n    # Use popen so we can pend on completion\n    result = subprocess.run(ffmpeg_args, stdout=subprocess.PIPE, stderr=subprocess.PIPE, universal_newlines=True, encoding='utf-8', errors='ignore')\n    if result.returncode != 0:\n        print(' '.join(ffmpeg_args))\n        print(result.stderr)\n        raise RuntimeError('ffmpeg returned error code {}'.format(result.returncode))\n    png_files = [f for f in os.listdir(output_dir) if f.endswith('.png')]\n    return sorted(png_files)\n\ndef concat_png_sequence(input_filename : str, png_dir : str, output_dir : str):\n    mime, subtype = mimetypes.guess_type(input_filename)[0].split('/')\n    framerate = get_input_framerate(input_filename)\n    file_basename = os.path.splitext(os.path.basename(input_filename))[0]\n    output_filename = os.path.join(output_dir,file_basename + ('.gif' if subtype == 'gif' else '.mp4'))\n    ffmpeg_args = [\"ffmpeg\", '-hide_banner', '-y', '-framerate', f'{framerate}', '-pattern_type', 'glob', '-i', os.path.join(png_dir,'*.png')]\n    if subtype != 'gif':\n        ffmpeg_args.extend(['-c:v', 'libx264', '-crf', '20'])\n    ffmpeg_args.append(output_filename)\n    result = subprocess.run(ffmpeg_args, stdout=subprocess.PIPE, stderr=subprocess.PIPE, universal_newlines=True, encoding='utf-8', errors='ignore')\n    if result.returncode != 0:\n        print(' '.join(ffmpeg_args))\n        print(result.stderr)\n        raise RuntimeError('ffmpeg returned error code {}'.format(result.returncode))\n    return output_filename\n\ndef load_inception_model(cpu_mode=False):\n    if cpu_mode:\n        tensorflow.config.set_visible_devices([], 'GPU')\n        print('Using CPU only mode.')\n    else:\n        gpus = tensorflow.config.list_physical_devices('GPU')\n        print('Num GPUs Available: ', len(gpus))\n        if gpus:\n            try:\n                for gpu in gpus:\n                    tensorflow.config.experimental.set_memory_growth(gpu, True)\n            except RuntimeError as e:\n                print(e)\n\n    base_model = tensorflow.keras.applications.InceptionV3(include_top=False, weights='imagenet')\n    names = ['mixed3', 'mixed5']\n    layers = [base_model.get_layer(name).output for name in names]\n    dream_model = tensorflow.keras.Model(inputs=base_model.input, outputs=layers)\n    return dream_model\n\ndef process_file_or_directory(args, dream_model):\n    print(f'Using DeepDream mode \"{args.mode}\", steps: {args.steps}, step size: {args.step_size}')\n    if 'random' in args.octaves:\n        num_octaves = int(args.octaves.split(' ')[-1])\n        octaves = [random.randint(args.rand_min, args.rand_max) for _ in range(num_octaves)]\n    elif 'range' in args.octaves:\n        _, lower, upper = args.octaves.split(' ')\n        octaves = range(int(lower), int(upper))\n    else:\n        octaves = [int(num) for num in args.octaves.split(',')]\n    if args.mode == 'octaves':\n        print(f'Octaves: {[octave for octave in octaves]}')\n    \n    os.makedirs(args.output, exist_ok=True)\n\n    # DIRECTORY PROCESSING\n    if os.path.isdir(args.input):\n        print(f'Processing directory {args.input} (Worker {args.worker_id}/{args.num_workers})')\n        files = sorted([f for f in os.listdir(args.input) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n        \n        # Partition files\n        my_files = files[args.worker_id::args.num_workers]\n        print(f'Assigned {len(my_files)} files out of {len(files)} total.')\n\n        output_dirname = os.path.join(args.output, os.path.basename(args.input.rstrip(os.sep)) + '-dream')\n        os.makedirs(output_dirname, exist_ok=True)\n\n        for filename in my_files:\n            infile = os.path.join(args.input, filename)\n            outfile = os.path.join(output_dirname, filename)\n            print(f'  {infile} -> {outfile}')\n            \n            original_img, img_size = load_img(infile, max_dim=args.max_size)\n            \n            if args.mode == 'simple':\n                dream_img = run_deep_dream_simple(original_img, dream_model, steps=args.steps, step_size=args.step_size)\n            elif args.mode == 'octaves':\n                dream_img = run_deep_dream_with_octaves(original_img, dream_model,\n                    steps_per_octave=args.steps,\n                    step_size=args.step_size,\n                    octave_scale=args.scale,\n                    tile_size=args.tile_size if args.tile_size is not None else max(img_size))\n            else:\n                raise RuntimeError(f'Unrecognized mode {args.mode}')\n            \n            save_img(dream_img, outfile)\n        print(f'Worker {args.worker_id} finished.')\n        if args.num_workers > 1: # Only exit if triggered as a worker process\n            sys.exit(0)\n\n    # SINGLE FILE PROCESSING\n    else:\n        guess = mimetypes.guess_type(args.input)[0]\n        if guess:\n            mime, subtype = guess.split('/')\n        else:\n            mime, subtype = None, None\n\n        if mime == 'image' and subtype != 'gif':\n            original_img, img_size = load_img(args.input, max_dim=args.max_size)\n            if args.mode == 'simple':\n                dream_img = run_deep_dream_simple(original_img, dream_model, steps=args.steps, step_size=args.step_size)\n            elif args.mode == 'octaves':\n                dream_img = run_deep_dream_with_octaves(original_img, dream_model,\n                    steps_per_octave=args.steps,\n                    step_size=args.step_size,\n                    octave_scale=args.scale,\n                    tile_size=args.tile_size if args.tile_size is not None else max(img_size))\n            else:\n                raise RuntimeError(f'Unrecognized mode {args.mode}')\n            output_basename = os.path.splitext(os.path.basename(args.input))[0]\n            output_filename = os.path.join(args.output,output_basename + '.png')\n            save_img(dream_img, output_filename)\n            print(f'Output rendered to {output_filename}')\n        \n        elif mime == 'video' or (mime == 'image' and subtype == 'gif'):\n            file_basename = os.path.splitext(os.path.basename(args.input))[0]\n            output_dirname = os.path.join(args.output, file_basename)\n            png_dir = os.path.join(output_dirname, 'pngs')\n            png_dream_dir = os.path.join(output_dirname, 'pngs-dream')\n            os.makedirs(png_dir, exist_ok=True)\n            os.makedirs(png_dream_dir, exist_ok=True)\n\n            print(f'Extracting frames to {png_dir}')\n            output_to_png_sequence(args.input, png_dir)\n            \n            files = sorted([f for f in os.listdir(png_dir) if f.endswith('.png')])\n            print(f'Processing {len(files)} frames...')\n\n            previous_img = None\n            for i, filename in enumerate(files):\n                infile = os.path.join(png_dir, filename)\n                outfile = os.path.join(png_dream_dir, filename)\n                \n                original_img, img_size = load_img(infile, max_dim=args.max_size)\n                \n                # Blend with previous frame if enabled\n                if args.blend > 0 and previous_img is not None:\n                     # Resize previous to match current (if video size changes, though unlikely for mp4)\n                     # Simple blend: input = current * (1-blend) + prev * blend\n                     # But deepdream logic usually blends *before* processing to create flow.\n                     # Here let's just use the logic as intended: modify 'original_img'\n                     # Note: Tensors in TF are immutable, need to act carefully.\n                     # load_img returns a tensor. \n                     # We can just do weighted average.\n                     original_img = original_img * (1.0 - args.blend) + previous_img * args.blend\n                \n                if args.mode == 'simple':\n                    dream_img = run_deep_dream_simple(original_img, dream_model, steps=args.steps, step_size=args.step_size)\n                elif args.mode == 'octaves':\n                    dream_img = run_deep_dream_with_octaves(original_img, dream_model,\n                        steps_per_octave=args.steps,\n                        step_size=args.step_size,\n                        octave_scale=args.scale,\n                        tile_size=args.tile_size if args.tile_size is not None else max(img_size))\n                \n                save_img(dream_img, outfile)\n                print(f'{infile} -> {outfile}', flush=True)\n                \n                # Update previous_img for next frame\n                # Optional: use dream_img or original_img? \n                # Flow works best if we feed the *result* back, or at least the hallucinated structure.\n                # Standard implementation feeds the *result*.\n                # But we need to ensure size matches.\n                previous_img = dream_img \n\n            print('Creating output video...')\n            output_video = os.path.join(args.output, file_basename + '-dream.mp4')\n            concat_png_sequence(args.input, png_dream_dir, output_video)\n            print(f'Video saved to {output_video}')\n        else:\n            print(f'Unsupported file type: {mime}/{subtype}')\n\nif __name__ == \"__main__\":\n    try:\n        parser = argparse.ArgumentParser(prog='DeepDream runner')\n        parser.add_argument('-i', '--input', type=str, default='example.png', help='Input file path (Image or Video) or Directory')\n        parser.add_argument('--output', type=str, default='output', help='Output directory path')\n        parser.add_argument('--mode', type=str, choices=['simple', 'octaves'], default='simple', help='DeepDream mode')\n        parser.add_argument('--octaves', type=str, default='-2,-1,0,1,2', help='Octaves/Layers to use. Examples: \"random 5\" (5 random layers), \"range 3 8\" (layers 3 to 7), \"-2,-1,0,1,2\" (specific layers)')\n        parser.add_argument('--rand_min', type=int, default=1, help='Min layer index for random octaves')\n        parser.add_argument('--rand_max', type=int, default=11, help='Max layer index for random octaves')\n        parser.add_argument('--scale', type=float, default=1.3, help='Scale factor for octaves')\n        parser.add_argument('--max_size', type=int, default=0, help='Max image dimension (0 = no limit)')\n        parser.add_argument('--blend', type=float, default=0.0, help='Amount to blend images when processing video')\n        parser.add_argument('--cpu', action='store_true', help='Use CPU only')\n        parser.add_argument('--diff', action='store_true', help='Use difference between original and dream image')\n        parser.add_argument('--steps', type=int, default=20, help='Total number of steps, or steps per octave if using \"octaves\" mode')\n        parser.add_argument('--step_size', type=float, default=0.1, help='Step size')\n        parser.add_argument('--tile_size', type=int, help='Tile size used for \"octaves\" mode. If not specified, the tile size is the max dimension of the input image.')\n        parser.add_argument('--worker_id', type=int, default=0, help='Worker ID for parallel processing (0-based)')\n        parser.add_argument('--num_workers', type=int, default=1, help='Total number of workers for parallel processing')\n\n        args, unknown_args = parser.parse_known_args()\n        \n        model = load_inception_model(args.cpu)\n        process_file_or_directory(args, model)\n    except Exception:\n        print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setup Input Source\n",
    "Choose to upload a file or mount Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import files\n",
    "from google.colab import drive\n",
    "\n",
    "source_type = \"Upload File\" #@param [\"Mount Google Drive\", \"Upload File\"]\n",
    "mount_path = \"/content/drive\" \n",
    "\n",
    "if source_type == \"Mount Google Drive\":\n",
    "    drive.mount(mount_path)\n",
    "    print(f\"Google Drive mounted at {mount_path}\")\n",
    "elif source_type == \"Upload File\":\n",
    "    uploaded = files.upload()\n",
    "    if uploaded:\n",
    "        # Taking the first uploaded file\n",
    "        fname = list(uploaded.keys())[0]\n",
    "        print(f\"File uploaded to: /content/{fname}\")\n",
    "        print(\"Please copy this path to the 'input_path' field below if it's not automatically set.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Preview Frame (Optional)\n",
    "Helper to extract a single frame from a video to check if the input path is correct or to decide on parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"example.png\" #@param {type:\"string\"}\n",
    "preview_frame_index = 0 #@param {type:\"integer\"}\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "from IPython.display import Image, display\n",
    "\n",
    "if not os.path.exists(input_path):\n",
    "    print(f\"Error: Input file at {input_path} not found.\")\n",
    "else:\n",
    "    # Check mime type simply by extension or assuming it's supported\n",
    "    import mimetypes\n",
    "    mime, _ = mimetypes.guess_type(input_path)\n",
    "    \n",
    "    if mime and mime.startswith('image') and not input_path.endswith('.gif'):\n",
    "        print(\"Input is an image, displaying directly:\")\n",
    "        display(Image(input_path))\n",
    "    else:\n",
    "        # Extract frame using ffmpeg\n",
    "        output_preview = \"preview.jpg\"\n",
    "        # -vf select='eq(n\\,{index})' -vframes 1\n",
    "        cmd = [\n",
    "            \"ffmpeg\", \"-y\", \"-hide_banner\", \"-loglevel\", \"error\",\n",
    "            \"-i\", input_path,\n",
    "            \"-vf\", f\"select='eq(n,{preview_frame_index})'\",\n",
    "            \"-vframes\", \"1\",\n",
    "            output_preview\n",
    "        ]\n",
    "        print(f\"Extracting frame {preview_frame_index}...\")\n",
    "        subprocess.run(cmd)\n",
    "        if os.path.exists(output_preview):\n",
    "             display(Image(output_preview))\n",
    "        else:\n",
    "             print(\"Failed to extract frame. Is the index out of bounds?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Load Model\n",
    "Load the InceptionV3 model once to speed up subsequent runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import dream\n",
    "    if 'dream_model' not in globals() or dream_model is None:\n",
    "        print(\"Loading InceptionV3 Model...\")\n",
    "        dream_model = dream.load_inception_model()\n",
    "        print(\"Model loaded successfully.\")\n",
    "    else:\n",
    "        print(\"Model already loaded.\")\n",
    "except ImportError:\n",
    "    print(\"Error: dream.py not found. Please ensure 'Deploy Code' step ran successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Run DeepDream\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_path = \"example.png\" #@param {type:\"string\"}\n",
    "output_dir = \"output\" #@param {type:\"string\"}\n",
    "\n",
    "#@markdown ### Processing Mode\n",
    "mode = \"octaves\" #@param [\"simple\", \"octaves\"]\n",
    "octaves = \"-2, -1, 0, 1, 2\" #@param {type:\"string\"}\n",
    "scale = 1.3 #@param {type:\"number\"}\n",
    "tile_size = 0 #@param {type:\"integer\"}\n",
    "\n",
    "#@markdown ### Iteration Settings\n",
    "steps = 20 #@param {type:\"integer\"}\n",
    "step_size = 0.1 #@param {type:\"number\"}\n",
    "\n",
    "#@markdown ### Advanced & Video\n",
    "blend = 0.0 #@param {type:\"number\"}\n",
    "diff = False #@param {type:\"boolean\"}\n",
    "max_size = 0 #@param {type:\"integer\"}\n",
    "use_cpu = False #@param {type:\"boolean\"}\n",
    "max_workers = 1 #@param {type:\"slider\", min:1, max:4, step:1}\n",
    "\n",
    "#@markdown ### Preview (Video only)\n",
    "preview_video_frame = False #@param {type:\"boolean\"}\n",
    "preview_frame_num = 1 #@param {type:\"integer\"}\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import re\n",
    "from IPython.display import Image, display\n",
    "from PIL import Image as PILImage\n",
    "import dream # Ensure available\n",
    "import argparse # Needed for constructing args object\n",
    "\n",
    "def get_video_info(filename):\n",
    "    if not os.path.exists(filename):\n",
    "        return None\n",
    "    cmd = [\"ffprobe\", \"-v\", \"error\", \"-select_streams\", \"v\", \"-show_entries\", \"stream=width,height,display_aspect_ratio,r_frame_rate\", \"-of\", \"csv=p=0\", filename]\n",
    "    try:\n",
    "        result = subprocess.run(cmd, stdout=subprocess.PIPE, text=True)\n",
    "        if result.returncode == 0:\n",
    "            parts = result.stdout.strip().split(',')\n",
    "            if len(parts) >= 2:\n",
    "                w, h = parts[0], parts[1]\n",
    "                ar = parts[2] if len(parts) > 2 else f\"{w}:{h}\"\n",
    "                fps_str = parts[3] if len(parts) > 3 else \"30/1\"\n",
    "                try:\n",
    "                    num, den = fps_str.split('/')\n",
    "                    fps = float(num) / float(den)\n",
    "                except:\n",
    "                    fps = 30.0\n",
    "                return w, h, ar, fps\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "if not os.path.exists(input_path):\n",
    "    print(f\"Error: Input file not found at {input_path}\")\n",
    "else:\n",
    "    # Check for model\n",
    "    if 'dream_model' not in globals():\n",
    "        print(\"Model not loaded. Attempting to load (this might take a moment)...\")\n",
    "        dream_model = dream.load_inception_model(use_cpu)\n",
    "\n",
    "    # Construct Args object to pass to dream.process_file_or_directory\n",
    "    # We simulate argparse namespace\n",
    "    class DreamArgs:\n",
    "        def __init__(self):\n",
    "             self.input = input_path\n",
    "             self.output = output_dir\n",
    "             self.mode = mode\n",
    "             self.octaves = octaves\n",
    "             self.scale = scale\n",
    "             self.steps = steps\n",
    "             self.step_size = step_size\n",
    "             self.tile_size = tile_size if tile_size > 0 else None\n",
    "             self.blend = blend\n",
    "             self.diff = diff\n",
    "             self.max_size = max_size\n",
    "             self.cpu = use_cpu\n",
    "             self.rand_min = 1\n",
    "             self.rand_max = 11\n",
    "             self.worker_id = 0\n",
    "             self.num_workers = 1\n",
    "\n",
    "    run_args = DreamArgs()\n",
    "\n",
    "    # Handle Preview Mode logic\n",
    "    if preview_video_frame:\n",
    "        if max_workers > 1:\n",
    "            print(\"Warning: Preview disabled in Multi-Worker mode.\")\n",
    "        else:\n",
    "            print(\"--- PREVIEW MODE ---\")\n",
    "            v_info = get_video_info(input_path)\n",
    "            if v_info:\n",
    "                 print(f\"Original Video: {v_info[0]}x{v_info[1]} (AR: {v_info[2]}, FPS: {v_info[3]:.2f})\")\n",
    "            \n",
    "            preview_img = \"preview_temp.png\"\n",
    "            print(f\"Extracting frame {preview_frame_num}...\")\n",
    "            frame_idx = max(0, preview_frame_num - 1)\n",
    "            # FIXED: Syntax Warning escape sequence\n",
    "            cmd_extract = [\n",
    "                \"ffmpeg\", \"-y\", \"-i\", input_path,\n",
    "                \"-vf\", f\"select='eq(n,{frame_idx})'\",\n",
    "                \"-vframes\", \"1\", preview_img\n",
    "            ]\n",
    "            subprocess.run(cmd_extract, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "            \n",
    "            if os.path.exists(preview_img):\n",
    "                print(\"Processing preview frame...\")\n",
    "                # Update args for preview\n",
    "                preview_args = DreamArgs()\n",
    "                preview_args.input = preview_img\n",
    "                # Use loaded model directly!\n",
    "                dream.process_file_or_directory(preview_args, dream_model)\n",
    "                \n",
    "                result_path = os.path.join(output_dir, \"preview_temp.png\")\n",
    "                if os.path.exists(result_path):\n",
    "                    with PILImage.open(result_path) as img:\n",
    "                        w, h = img.size\n",
    "                        ar = f\"{w/h:.2f}\"\n",
    "                        print(f\"Processed Frame: {w}x{h} (AR: ~{ar})\")\n",
    "                    display(Image(result_path))\n",
    "                else:\n",
    "                    print(\"Error: Preview processing failed.\")\n",
    "            else:\n",
    "                print(\"Error: Could not extract frame.\")\n",
    "            # No sys.exit(0) here because we are in main process, just stop doing other things\n",
    "\n",
    "    # Multi-Worker Setup (Still uses subprocess for isolation/GIL)\n",
    "    elif max_workers > 1:\n",
    "        # Check if input is video\n",
    "        import mimetypes\n",
    "        fake_mime, _ = mimetypes.guess_type(input_path)\n",
    "        if fake_mime and fake_mime.startswith('image') and not input_path.endswith('.gif'):\n",
    "            print(\"Multi-worker mode ignored for single image input.\")\n",
    "            max_workers = 1\n",
    "            # Fallthrough to Normal Processing\n",
    "            is_multi_worker = False\n",
    "\n",
    "        else:\n",
    "             is_multi_worker = True\n",
    "             if blend > 0:\n",
    "                 print(\"Warning: Blend > 0 is incompatible with Multi-Worker mode. Enforcing blend=0.\")\n",
    "                 blend = 0.0\n",
    "\n",
    "             print(f\"--- MULTI-WORKER MODE ({max_workers} Parallel Processes) ---\")\n",
    "             \n",
    "             # 1. Extract Frames\n",
    "             v_info = get_video_info(input_path)\n",
    "             fps = v_info[3] if v_info else 30.0\n",
    "             print(f\"Input FPS: {fps:.2f}\")\n",
    "             \n",
    "             # Create structured output folders: output/video_name/pngs\n",
    "             base_name = os.path.splitext(os.path.basename(input_path))[0]\n",
    "             video_output_dir = os.path.join(output_dir, base_name)\n",
    "             temp_frames_dir = os.path.join(video_output_dir, \"pngs\") \n",
    "             processed_dir = os.path.join(video_output_dir, \"pngs-dream\")\n",
    "             \n",
    "             if os.path.exists(temp_frames_dir): shutil.rmtree(temp_frames_dir)\n",
    "             if os.path.exists(processed_dir): shutil.rmtree(processed_dir)\n",
    "             os.makedirs(temp_frames_dir, exist_ok=True)\n",
    "             os.makedirs(processed_dir, exist_ok=True)\n",
    "             \n",
    "             print(\"Extracting frames for parallel processing...\")\n",
    "             cmd_extract = [\"ffmpeg\", \"-hide_banner\", \"-loglevel\", \"error\", \"-i\", input_path, os.path.join(temp_frames_dir, \"%05d.png\")]\n",
    "             subprocess.run(cmd_extract, check=True)\n",
    "             \n",
    "             # 2. Launch Workers\n",
    "             print(f\"Launching {max_workers} 'dream.py' workers...\")\n",
    "             procs = []\n",
    "             for i in range(max_workers):\n",
    "                 # Point workers to write to the correct processed_dir directly?\n",
    "                 # dream.py logic says: output_dirname = os.path.join(args.output, base + '-dream') if dir input\n",
    "                 # If input is .../pngs, output will be .../pngs-dream. Perfect.\n",
    "                 # So we pass video_output_dir as 'output', and input as 'temp_frames_dir'.\n",
    "                 # dream.py will create video_output_dir/pngs-dream\n",
    "                 \n",
    "                 cmd = [\"python\", \"-u\", \"dream.py\", \"-i\", temp_frames_dir, \"--output\", video_output_dir]\n",
    "                 # Add common params\n",
    "                 cmd.extend([\"--mode\", mode, \"--worker_id\", str(i), \"--num_workers\", str(max_workers)])\n",
    "                 if mode == \"octaves\":\n",
    "                     cmd.extend([\"--octaves\", octaves, \"--scale\", str(scale)])\n",
    "                     if tile_size > 0: cmd.extend([\"--tile_size\", str(tile_size)])\n",
    "                 cmd.extend([\"--steps\", str(steps), \"--step_size\", str(step_size)])\n",
    "                 if max_size > 0: cmd.extend([\"--max_size\", str(max_size)])\n",
    "                 if use_cpu: cmd.append(\"--cpu\")\n",
    "                 \n",
    "                 # Use PIPE to avoid fileno error in Colab\n",
    "                 p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, bufsize=1, universal_newlines=True)\n",
    "                 procs.append(p)\n",
    "             \n",
    "             # Monitor workers and print output\n",
    "             import time\n",
    "             try:\n",
    "                 while any(p.poll() is None for p in procs):\n",
    "                     for i, p in enumerate(procs):\n",
    "                         # Read available lines\n",
    "                         while True:\n",
    "                             line = p.stdout.readline()\n",
    "                             if not line: break\n",
    "                             print(f\"[Worker {i}] {line.strip()}\")\n",
    "                     time.sleep(0.1)\n",
    "             except KeyboardInterrupt:\n",
    "                 print(\"\\nInterrupted! Killing workers...\")\n",
    "                 for p in procs:\n",
    "                     p.terminate()\n",
    "                 # Wait a bit for them to die\n",
    "                 time.sleep(1)\n",
    "                 for p in procs:\n",
    "                     if p.poll() is None:\n",
    "                         p.kill()\n",
    "                 print(\"Workers terminated.\")\n",
    "                 raise # Re-raise to stop cell execution\n",
    "             \n",
    "             # Final flush\n",
    "             for i, p in enumerate(procs):\n",
    "                 for line in p.stdout:\n",
    "                     print(f\"[Worker {i}] {line.strip()}\")\n",
    "                 if p.returncode != 0:\n",
    "                     print(f\"Worker {i} failed with code {p.returncode}\")\n",
    "             \n",
    "             print(\"All workers finished.\")\n",
    "             \n",
    "             # 3. Combine Frames\n",
    "             # dream.py creates folder \"input_dirname-dream\" inside output dir.\n",
    "             # input was \"video_output_dir/pngs\", so output is \"video_output_dir/pngs-dream\"\n",
    "             # which matches 'processed_dir' variable we defined above.\n",
    "             \n",
    "             if not os.path.exists(processed_dir):\n",
    "                 print(f\"Error: Processed frames directory not found at {processed_dir}. Did workers fail?\")\n",
    "             else:\n",
    "                 print(\"Combining processed frames...\")\n",
    "                 output_filename = os.path.join(output_dir, base_name + \"-dream.mp4\")\n",
    "                 cmd_combine = [\n",
    "                     \"ffmpeg\", \"-y\", \"-framerate\", str(fps), \"-pattern_type\", \"glob\",\n",
    "                     \"-i\", os.path.join(processed_dir, \"*.png\"),\n",
    "                     \"-c:v\", \"libx264\", \"-pix_fmt\", \"yuv420p\",\n",
    "                     \"-vf\", \"pad=ceil(iw/2)*2:ceil(ih/2)*2\", # Fix div-by-2 error\n",
    "                     output_filename\n",
    "                 ]\n",
    "                 subprocess.run(cmd_combine, check=True)\n",
    "                 print(f\"Processing Complete. Video saved to {output_filename}\")\n",
    "\n",
    "    # Fallback Logic Check: execute only if NOT multi-worker AND not preview\n",
    "    # We initialize is_multi_worker = False at top of this cell implicitly by not setting it? \n",
    "    # Better to rely on locals() check carefully or just 'else' branch? \n",
    "    # But previous block was elif.\n",
    "    # Logic:\n",
    "    # if preview: ...\n",
    "    # elif max_workers > 1: ... (sets is_multi_worker=True unless single image)\n",
    "    # \n",
    "    # We need a final block that runs for single worker.\n",
    "    # It should run if:\n",
    "    # 1. Not preview\n",
    "    # 2. AND (max_workers == 1 OR (max_workers > 1 but downgraded to single image))\n",
    "    \n",
    "    should_run_single = False\n",
    "    if not preview_video_frame:\n",
    "        if max_workers == 1:\n",
    "            should_run_single = True\n",
    "        elif 'is_multi_worker' in locals() and not is_multi_worker:\n",
    "             should_run_single = True\n",
    "        # If is_multi_worker is True, we do NOT run single.\n",
    "        \n",
    "    if should_run_single:\n",
    "        # --- STANDARD SINGLE WORKER MODE (Video or Image) ---\n",
    "        # Use imported dream module for speed!\n",
    "        print(f\"Processing {input_path} in single-process mode...\")\n",
    "        dream.process_file_or_directory(run_args, dream_model)\n",
    "        \n",
    "        print(\"\\nProcessing Complete.\")\n",
    "        # Check if it was a single image and display it\n",
    "        import mimetypes\n",
    "        mime, subtype = mimetypes.guess_type(input_path)\n",
    "        if mime and mime.startswith('image') and not subtype == 'gif':\n",
    "             base = os.path.splitext(os.path.basename(input_path))[0]\n",
    "             out_file = os.path.join(output_dir, base + '.png')\n",
    "             if os.path.exists(out_file):\n",
    "                 print(\"Result:\")\n",
    "                 display(Image(out_file))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Create Video from Frames (Optional)\n",
    "Run this cell if you want to manually create a video from a folder of images. Useful if `dream.py` stopped early or if you want to adjust framerate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_folder = \"output/example\" #@param {type:\"string\"}\n",
    "output_video_filename = \"output/dream.mp4\" #@param {type:\"string\"}\n",
    "#@markdown Leave fps as 0 to attempt auto-detect from input_path (if it exists)\n",
    "fps = 0 #@param {type:\"integer\"}\n",
    "input_path_for_fps = \"example.png\" #@param {type:\"string\"}\n",
    "transfer_audio = True #@param {type:\"boolean\"}\n",
    "#@markdown ### Resolution\n",
    "#@markdown -1: Keep processed resolution (safest). 0: Match original input resolution. >0: Scale width (keep aspect).\n",
    "video_resolution = -1 #@param {type:\"integer\"}\n",
    "\n",
    "#@markdown ### Save to Drive\n",
    "save_to_drive = False #@param {type:\"boolean\"}\n",
    "drive_output_path = \"/content/drive/MyDrive\" #@param {type:\"string\"}\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import shutil\n",
    "\n",
    "def get_fps(filename):\n",
    "    if not filename or not os.path.exists(filename):\n",
    "        return None\n",
    "    cmd = [\"ffprobe\", \"-v\", \"error\", \"-select_streams\", \"v\", \"-of\", \"default=noprint_wrappers=1:nokey=1\", \"-show_entries\", \"stream=r_frame_rate\", filename]\n",
    "    try:\n",
    "        result = subprocess.run(cmd, stdout=subprocess.PIPE, text=True)\n",
    "        if result.returncode == 0:\n",
    "            num, den = result.stdout.strip().split('/')\n",
    "            return float(num) / float(den)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def get_resolution(filename):\n",
    "    if not filename or not os.path.exists(filename):\n",
    "        return None\n",
    "    cmd = [\"ffprobe\", \"-v\", \"error\", \"-select_streams\", \"v\", \"-of\", \"csv=p=0\", \"-show_entries\", \"stream=width,height\", filename]\n",
    "    try:\n",
    "        result = subprocess.run(cmd, stdout=subprocess.PIPE, text=True)\n",
    "        if result.returncode == 0:\n",
    "            w, h = map(int, result.stdout.strip().split(','))\n",
    "            return w, h\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def has_audio_stream(filename):\n",
    "    if not filename or not os.path.exists(filename):\n",
    "        return False\n",
    "    cmd = [\"ffprobe\", \"-v\", \"error\", \"-select_streams\", \"a\", \"-show_entries\", \"stream=index\", \"-of\", \"csv=p=0\", filename]\n",
    "    try:\n",
    "        result = subprocess.run(cmd, stdout=subprocess.PIPE, text=True)\n",
    "        if result.returncode == 0 and len(result.stdout.strip()) > 0:\n",
    "            return True\n",
    "    except Exception:\n",
    "        pass\n",
    "    return False\n",
    "\n",
    "if not os.path.exists(frames_folder):\n",
    "    print(f\"Error: Folder {frames_folder} not found.\")\n",
    "else:\n",
    "    # Check if folder has pngs\n",
    "    pngs = [f for f in os.listdir(frames_folder) if f.endswith('.png')]\n",
    "    if not pngs:\n",
    "        print(f\"No .png files found in {frames_folder}\")\n",
    "    else:\n",
    "        print(f\"Found {len(pngs)} frames.\")\n",
    "        \n",
    "        target_fps = fps\n",
    "        if target_fps <= 0:\n",
    "            # Try to detect\n",
    "            detected = get_fps(input_path_for_fps)\n",
    "            if detected:\n",
    "                print(f\"Auto-detected FPS from {input_path_for_fps}: {detected}\")\n",
    "                target_fps = detected\n",
    "            else:\n",
    "                print(\"Could not auto-detect FPS, defaulting to 30.\")\n",
    "                target_fps = 30\n",
    "        else:\n",
    "             print(f\"Using manual FPS: {target_fps}\")\n",
    "\n",
    "        print(\"Combining frames...\")\n",
    "        \n",
    "        # Build ffmpeg command\n",
    "        cmd = [\"ffmpeg\", \"-y\"]\n",
    "        \n",
    "        # Input 0: Frames with sequence pattern\n",
    "        cmd.extend([\n",
    "            \"-f\", \"image2\",\n",
    "            \"-framerate\", str(target_fps),\n",
    "            \"-start_number\", \"1\",\n",
    "            \"-i\", os.path.join(frames_folder, \"%05d.png\")\n",
    "        ])\n",
    "        \n",
    "        # Input 1: Audio (original video)\n",
    "        audio_source = None\n",
    "        if transfer_audio and has_audio_stream(input_path_for_fps):\n",
    "            audio_source = input_path_for_fps\n",
    "            print(f\"Audio detected in {audio_source}, will copy to output.\")\n",
    "            # Input 2: Audio Source\n",
    "            cmd.extend([\"-i\", audio_source])\n",
    "        \n",
    "        # Output Options\n",
    "        cmd.extend([\"-c:v\", \"libx264\", \"-pix_fmt\", \"yuv420p\"])\n",
    "        \n",
    "        # Filters (Resolution & Pad)\n",
    "        filters = []\n",
    "        \n",
    "        if video_resolution == 0:\n",
    "             # Match original\n",
    "             orig_dims = get_resolution(input_path_for_fps)\n",
    "             if orig_dims:\n",
    "                 w, h = orig_dims\n",
    "                 print(f\"Scaling to match original resolution: {w}x{h}\")\n",
    "                 filters.append(f\"scale={w}:{h}\")\n",
    "             else:\n",
    "                 print(\"Could not determine original resolution to match. Using default.\")\n",
    "        elif video_resolution > 0:\n",
    "             print(f\"Scaling width to {video_resolution} (keeping aspect)\")\n",
    "             filters.append(f\"scale={video_resolution}:-2\")\n",
    "        \n",
    "        # Always apply pad to ensure mult of 2\n",
    "        filters.append(\"pad=ceil(iw/2)*2:ceil(ih/2)*2\")\n",
    "        \n",
    "        cmd.extend([\"-vf\", \",\".join(filters)])\n",
    "        \n",
    "        if audio_source:\n",
    "            # Map video from stream 0, audio from stream 1\n",
    "            cmd.extend([\"-map\", \"0:v\", \"-map\", \"1:a\", \"-c:a\", \"copy\", \"-shortest\"])\n",
    "        \n",
    "        cmd.append(output_video_filename)\n",
    "\n",
    "        print(f\"Executing: {' '.join(cmd)}\")\n",
    "        process = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, universal_newlines=True)\n",
    "        if process.returncode == 0:\n",
    "            print(f\"Video created successfully: {output_video_filename}\")\n",
    "            if save_to_drive:\n",
    "                if not os.path.exists(drive_output_path):\n",
    "                    print(f\"Drive path {drive_output_path} does not exist. Attempting to create...\")\n",
    "                    try:\n",
    "                         os.makedirs(drive_output_path, exist_ok=True)\n",
    "                    except Exception as e:\n",
    "                         print(f\"Could not create Drive directory: {e}\")\n",
    "                \n",
    "                if os.path.exists(drive_output_path):\n",
    "                    try:\n",
    "                        dest_file = os.path.join(drive_output_path, os.path.basename(output_video_filename))\n",
    "                        shutil.copy2(output_video_filename, dest_file)\n",
    "                        print(f\"Saved to Drive: {dest_file}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error saving to Drive: {e}\")\n",
    "                else:\n",
    "                     print(\"Skipping Drive save due to invalid path.\")\n",
    "\n",
    "        else:\n",
    "            print(\"Error creating video:\")\n",
    "            print(process.stderr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. View/Download Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import files\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Try to find the output image\n",
    "input_basename = os.path.splitext(os.path.basename(input_path))[0]\n",
    "possible_output = os.path.join(output_dir, input_basename + \".png\")\n",
    "\n",
    "if os.path.exists(possible_output):\n",
    "    print(f\"Displaying result: {possible_output}\")\n",
    "    display(Image(possible_output))\n",
    "    \n",
    "    # Checkbox to download\n",
    "    download_result = True #@param {type:\"boolean\"}\n",
    "    if download_result:\n",
    "        files.download(possible_output)\n",
    "else:\n",
    "    print(f\"Output file {possible_output} not found. (If you processed a video, check the output folder manually)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}